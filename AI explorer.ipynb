{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27df169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Q-table:\n",
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import time\n",
    "\n",
    "#Hyperparameters:\n",
    "learning_rate = 0.1  \n",
    "discount_factor = 0.99  \n",
    "exploration_rate = 0.1  \n",
    "num_episodes = 1000\n",
    "\n",
    "#Parameters:\n",
    "N_STATES = 6  \n",
    "ACTIONS = ['left', 'right']\n",
    "EPSILON = 0.9 \n",
    "ALPHA = 0.1 \n",
    "GAMMA = 0.9\n",
    "MAX_EPOCHES = 13\n",
    "FRESH_TIME = 0.3\n",
    "\n",
    "#Initializing the Q-table:\n",
    "q_table = pd.DataFrame(\n",
    "    np.zeros((N_STATES, len(ACTIONS))),\n",
    "    columns=ACTIONS\n",
    ")\n",
    "\n",
    "print(\"Initialized Q-table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751b9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_q_table(n_states, actions):\n",
    "    \"\"\"\n",
    "    Build a Q-table with all zero initial values.\n",
    "\n",
    "    Parameters:\n",
    "    - n_states (int): Number of states\n",
    "    - actions (list): List of possible actions\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Initialized Q-table\n",
    "    \"\"\"\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),  \n",
    "        columns=actions\n",
    "    )\n",
    "    return table\n",
    "\n",
    "#Building the Q-table:\n",
    "q_table = build_q_table(N_STATES, ACTIONS)\n",
    "\n",
    "#Printing the initialized Q-table:\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b91f03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    }
   ],
   "source": [
    "def choose_action(state, q_table):\n",
    "\n",
    "    random_value = np.random.rand()\n",
    "\n",
    "    #Explore vs non-explored states:\n",
    "    if random_value > EPSILON or q_table.loc[state].max() == 0: \n",
    "        action = np.random.choice(ACTIONS)\n",
    "    else:  \n",
    "        action = q_table.loc[state].idxmax()\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "sample_action = choose_action(0, q_table)\n",
    "print(sample_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f764ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_feedback(S_current, A):\n",
    "#Setting reward 'R' for correct exploration:\n",
    "    if A == 'right':\n",
    "        if S_current == N_STATES - 2:  \n",
    "            S_next = 'terminal' \n",
    "            R = 1 \n",
    "        else:\n",
    "            S_next = S_current + 1\n",
    "            R = 0\n",
    "    elif A == 'left':\n",
    "        if S_current == 0:\n",
    "            S_next = 0\n",
    "        else:\n",
    "            S_next = S_current - 1\n",
    "        R = 0\n",
    "\n",
    "    return S_next, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd6f147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0)\n"
     ]
    }
   ],
   "source": [
    "def update_env(S, episode, step_counter):\n",
    "\n",
    "    env_list = ['-'] * (N_STATES - 1) + ['T'] \n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode {}: total_steps = {}'.format(episode + 1, step_counter)\n",
    "        print('{}\\n'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "\n",
    "sample_action = 'left'\n",
    "S_current = 4\n",
    "sample_feedback = get_env_feedback(S_current, sample_action)\n",
    "print(sample_feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b78118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----oTEpisode 1: total_steps = 24\n",
      "----oTEpisode 2: total_steps = 5\n",
      "----oTEpisode 3: total_steps = 6\n",
      "----oTEpisode 4: total_steps = 7\n",
      "----oTEpisode 5: total_steps = 5\n",
      "----oTEpisode 6: total_steps = 5\n",
      "----oTEpisode 7: total_steps = 5\n",
      "----oTEpisode 8: total_steps = 5\n",
      "----oTEpisode 9: total_steps = 5\n",
      "----oTEpisode 10: total_steps = 5\n",
      "----oTEpisode 11: total_steps = 5\n",
      "----oTEpisode 12: total_steps = 5\n",
      "----oTEpisode 13: total_steps = 5\n",
      "\n",
      "Q-table:\n",
      "\n",
      "   left     right\n",
      "0   0.0  0.004239\n",
      "1   0.0  0.024903\n",
      "2   0.0  0.108445\n",
      "3   0.0  0.340790\n",
      "4   0.0  0.745813\n",
      "5   0.0  0.000000\n"
     ]
    }
   ],
   "source": [
    "def reinforce_learning():\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "\n",
    "    #Start training:\n",
    "    for episode in range(MAX_EPOCHES):\n",
    "        step_counter = 0\n",
    "        S_current = 0\n",
    "        is_terminated = False\n",
    "\n",
    "        update_env(S_current, episode, step_counter) \n",
    "\n",
    "        while not is_terminated:\n",
    "            A = choose_action(S_current, q_table)\n",
    "            S_next, R = get_env_feedback(S_current, A) \n",
    "\n",
    "            #Predicting the future reward:\n",
    "            q_predict = q_table.loc[S_current, A]\n",
    "\n",
    "            if S_next != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.loc[S_next].max()\n",
    "            else:\n",
    "                q_target = R\n",
    "                is_terminated = True\n",
    "\n",
    "            #Q Table updation with new knowledge of surroundings:\n",
    "            q_table.loc[S_current, A] += ALPHA * (q_target - q_predict)\n",
    "\n",
    "            S_current = S_next\n",
    "            step_counter += 1\n",
    "            update_env(S_current, episode, step_counter)\n",
    "\n",
    "    return q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = reinforce_learning()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8281f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
